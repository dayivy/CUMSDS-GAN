{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-06T11:14:14.55693Z","iopub.execute_input":"2022-06-06T11:14:14.557475Z","iopub.status.idle":"2022-06-06T11:14:14.583469Z","shell.execute_reply.started":"2022-06-06T11:14:14.557395Z","shell.execute_reply":"2022-06-06T11:14:14.582629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Brief Description of Problem\n\nIn this assignment we are going to try to use a GAN to create fake images of dogs. I will look at the size/shape of the data and any necessary cleaning procedures during EDA.  This challenge was the original Kaggle introduction to GANs until the competition closed recently.\n","metadata":{}},{"cell_type":"markdown","source":"# EDA\n\nLet's load the data and take a look at a few sample pictures.  Also, I want to look at the annotations file and see if it is anything necessary for this assignment.","metadata":{}},{"cell_type":"code","source":"import zipfile\n\nDataset = \"all-dogs\"\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/generative-dog-images/\"+Dataset+\".zip\",\"r\") as z:\n    z.extractall(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:14:14.58484Z","iopub.execute_input":"2022-06-06T11:14:14.585196Z","iopub.status.idle":"2022-06-06T11:14:31.445894Z","shell.execute_reply.started":"2022-06-06T11:14:14.585161Z","shell.execute_reply":"2022-06-06T11:14:31.444986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/working/all-dogs/'\nimages = os.listdir(PATH)\nprint(f'There are {len(images)} pictures of dogs.')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:14:31.45009Z","iopub.execute_input":"2022-06-06T11:14:31.45088Z","iopub.status.idle":"2022-06-06T11:14:31.472637Z","shell.execute_reply.started":"2022-06-06T11:14:31.450839Z","shell.execute_reply":"2022-06-06T11:14:31.471773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12,10))\n\nfor indx, axis in enumerate(axes.flatten()):\n    rnd_indx = np.random.randint(0, len(os.listdir(PATH)))\n    img = plt.imread(PATH + images[rnd_indx])\n    imgplot = axis.imshow(img)\n    axis.set_title(images[rnd_indx])\n    axis.set_axis_off()\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:14:31.473839Z","iopub.execute_input":"2022-06-06T11:14:31.474465Z","iopub.status.idle":"2022-06-06T11:14:32.709272Z","shell.execute_reply.started":"2022-06-06T11:14:31.474425Z","shell.execute_reply":"2022-06-06T11:14:32.705778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we see a few example pictures of the dogs.  These all appear to be fairly high quality images.  Also, some picture include humans and other objects.  \n\nGiven that these are high quality images, I want to see how many pixels are in each picture and find if they are consistent or not.","metadata":{}},{"cell_type":"code","source":"img_shapes = set()\n\nimgs = os.listdir(PATH)\n\nfor i in range(len(imgs)):\n    curr_img = plt.imread(PATH + imgs[i])\n    img_shapes.add(curr_img.shape)\n\nlen(img_shapes)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:14:32.711664Z","iopub.execute_input":"2022-06-06T11:14:32.712313Z","iopub.status.idle":"2022-06-06T11:15:37.541603Z","shell.execute_reply.started":"2022-06-06T11:14:32.712277Z","shell.execute_reply":"2022-06-06T11:15:37.540822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we see there are over 4500 different image shapes.  This would be a problem if we wanted to use the raw images since they are of various different sizes.  The best method to handle this would be to convert all images to a standard size.  I will initially use 64x64 images (in fact, it is really 64x64x3 for RGB coloring)\n\nNext, lets look at the annotation file.","metadata":{}},{"cell_type":"code","source":"import zipfile\n\nDataset = \"Annotation\"\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/generative-dog-images/\"+Dataset+\".zip\",\"r\") as z:\n    z.extractall(\".\")\n\n\nPATH_2 = '/kaggle/working/Annotation/'\nimages = os.listdir(PATH_2)\nlen(images)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:37.542806Z","iopub.execute_input":"2022-06-06T11:15:37.544333Z","iopub.status.idle":"2022-06-06T11:15:42.935316Z","shell.execute_reply.started":"2022-06-06T11:15:37.544285Z","shell.execute_reply":"2022-06-06T11:15:42.93437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    rnd_indx = np.random.randint(0, len(os.listdir(PATH_2)))\n    print(images[rnd_indx])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:42.939543Z","iopub.execute_input":"2022-06-06T11:15:42.941677Z","iopub.status.idle":"2022-06-06T11:15:42.94992Z","shell.execute_reply.started":"2022-06-06T11:15:42.941637Z","shell.execute_reply":"2022-06-06T11:15:42.949134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see these are just the dog breeds.  I will not be using this for this assignment.  This may have been necessary in the original competition, but it is closed now so I cant submit anything anyway.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing and Model Architecture","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:42.951095Z","iopub.execute_input":"2022-06-06T11:15:42.955039Z","iopub.status.idle":"2022-06-06T11:15:48.331065Z","shell.execute_reply.started":"2022-06-06T11:15:42.954981Z","shell.execute_reply":"2022-06-06T11:15:48.330115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_reshaped = keras.preprocessing.image_dataset_from_directory(\n    PATH, label_mode=None, image_size=(64, 64), batch_size=32\n)\ndataset_reshaped = dataset_reshaped.map(lambda x: x / 255.0)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:48.33521Z","iopub.execute_input":"2022-06-06T11:15:48.337856Z","iopub.status.idle":"2022-06-06T11:15:51.772926Z","shell.execute_reply.started":"2022-06-06T11:15:48.337826Z","shell.execute_reply":"2022-06-06T11:15:51.771239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor x in dataset_reshaped:\n    plt.axis(\"off\")\n    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:51.774464Z","iopub.execute_input":"2022-06-06T11:15:51.774844Z","iopub.status.idle":"2022-06-06T11:15:52.163538Z","shell.execute_reply.started":"2022-06-06T11:15:51.774792Z","shell.execute_reply":"2022-06-06T11:15:52.162697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, I have reshaped each image into 64x64x3.  This allows for consistency to use as the input shape in our model.\n\nNext, I will create the discriminator.  I struggled with developing my own architecture that seemingly worked in any sort of timely fashion, so I initially used the architecture as seen in our lecture and found on the keras tutorial page <https://keras.io/examples/generative/dcgan_overriding_train_step/#dcgan-to-generate-face-images>","metadata":{}},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"code","source":"discriminator = keras.Sequential(\n    [\n        keras.Input(shape=(64, 64, 3)),\n        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Flatten(),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ],\n    name=\"discriminator\",\n)\ndiscriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:52.166417Z","iopub.execute_input":"2022-06-06T11:15:52.167231Z","iopub.status.idle":"2022-06-06T11:15:52.436673Z","shell.execute_reply.started":"2022-06-06T11:15:52.167192Z","shell.execute_reply":"2022-06-06T11:15:52.43588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now for the generator model.","metadata":{}},{"cell_type":"code","source":"latent_dim = 128\n\ngenerator = keras.Sequential(\n    [\n        keras.Input(shape=(latent_dim,)),\n        layers.Dense(8 * 8 * 128),\n        layers.Reshape((8, 8, 128)),\n        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n    ],\n    name=\"generator\",\n)\ngenerator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:52.437946Z","iopub.execute_input":"2022-06-06T11:15:52.438474Z","iopub.status.idle":"2022-06-06T11:15:52.511012Z","shell.execute_reply.started":"2022-06-06T11:15:52.438436Z","shell.execute_reply":"2022-06-06T11:15:52.510271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, I used the example code to combine the models while making one simple adjustment.  I removed the code for adding random noise to the labels.  While this is denoted as an important trick, this seemed to make my models extremely unstable and made it seemingly impossible to learn.  Perhaps this step would be useful in more standardized sets (like CIFAR-10) but the fact the images we are given often have other objects in them seemed to create too much confusion when random noise was applied to the true labels.","metadata":{}},{"cell_type":"code","source":"class GAN(keras.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(GAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super(GAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n\n    @property\n    def metrics(self):\n        return [self.d_loss_metric, self.g_loss_metric]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n\n        # Decode them to fake images\n        generated_images = self.generator(random_latent_vectors)\n\n        # Combine them with real images\n        combined_images = tf.concat([generated_images, real_images], axis=0)\n\n        # Assemble labels discriminating real from fake images\n        labels = tf.concat(\n            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n        )\n        # Add random noise to the labels - important trick!\n        #labels += 0.05 * tf.random.uniform(tf.shape(labels))\n\n        # Train the discriminator\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        # Sample random points in the latent space\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n\n        # Assemble labels that say \"all real images\"\n        misleading_labels = tf.zeros((batch_size, 1))\n\n        # Train the generator (note that we should *not* update the weights\n        # of the discriminator)!\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(self.generator(random_latent_vectors))\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n\n        \n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        return {\n            \"d_loss\": self.d_loss_metric.result(),\n            \"g_loss\": self.g_loss_metric.result(),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:52.512165Z","iopub.execute_input":"2022-06-06T11:15:52.512501Z","iopub.status.idle":"2022-06-06T11:15:52.528244Z","shell.execute_reply.started":"2022-06-06T11:15:52.512466Z","shell.execute_reply":"2022-06-06T11:15:52.52748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I also added callbacks to the code, showing a few images after each epoch to see how the model is improving.  This is interesting, but the first few epochs probably wont show too much learning.","metadata":{}},{"cell_type":"code","source":"class GANMonitor(keras.callbacks.Callback):\n    def __init__(self, num_img=3, latent_dim=128):\n        self.num_img = num_img\n        self.latent_dim = latent_dim\n\n    def on_epoch_end(self, epoch, logs=None):\n        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n        generated_images = self.model.generator(random_latent_vectors)\n        generated_images *= 255\n        generated_images.numpy()\n        for i in range(self.num_img):\n            img = keras.preprocessing.image.array_to_img(generated_images[i])\n            plt.imshow(img)\n            plt.figure(i+1)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:52.529467Z","iopub.execute_input":"2022-06-06T11:15:52.529941Z","iopub.status.idle":"2022-06-06T11:15:52.53978Z","shell.execute_reply.started":"2022-06-06T11:15:52.529903Z","shell.execute_reply":"2022-06-06T11:15:52.539021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5 \n\ngan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\ngan.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss_fn=keras.losses.BinaryCrossentropy(),\n)\n\ngan.fit(\n    dataset_reshaped, epochs=epochs,callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:15:52.541801Z","iopub.execute_input":"2022-06-06T11:15:52.542066Z","iopub.status.idle":"2022-06-06T11:26:51.363389Z","shell.execute_reply.started":"2022-06-06T11:15:52.542041Z","shell.execute_reply":"2022-06-06T11:26:51.362461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, you cant really determine too much from the output of the first few epochs.  Mostly just pixelated blobs. ","metadata":{}},{"cell_type":"code","source":"class GANMonitor(keras.callbacks.Callback):\n    def __init__(self, num_img=3, latent_dim=128):\n        self.num_img = num_img\n        self.latent_dim = latent_dim\n\n    def on_epoch_end(self, epoch, logs=None):\n        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n        generated_images = self.model.generator(random_latent_vectors)\n        generated_images *= 255\n        generated_images.numpy()\n        for i in range(self.num_img):\n            img = keras.preprocessing.image.array_to_img(generated_images[i])\n            plt.imshow(img)\n            plt.figure(i+1)\n        #removed plt.show() so that images will now only print to the console when all epochs are complete","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:26:51.364681Z","iopub.execute_input":"2022-06-06T11:26:51.365122Z","iopub.status.idle":"2022-06-06T11:26:51.37233Z","shell.execute_reply.started":"2022-06-06T11:26:51.365079Z","shell.execute_reply":"2022-06-06T11:26:51.371392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 50\n\ngan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\ngan.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss_fn=keras.losses.BinaryCrossentropy(),\n)\n\ngan.fit(\n    dataset_reshaped, epochs=epochs,callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T11:26:51.374816Z","iopub.execute_input":"2022-06-06T11:26:51.375516Z","iopub.status.idle":"2022-06-06T13:02:04.106282Z","shell.execute_reply.started":"2022-06-06T11:26:51.375479Z","shell.execute_reply":"2022-06-06T13:02:04.105457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After 50 epochs, we are starting to see some more defined shapes forming.  This is interesting but we still can not make out much.  Lesser quality images should train faster so I will experiment with 32x32x3 images and many more epochs.","metadata":{}},{"cell_type":"code","source":"dataset_reshaped_LO = keras.preprocessing.image_dataset_from_directory(\n    PATH, label_mode=None, image_size=(32, 32), batch_size=32\n)\ndataset_reshaped_LO = dataset_reshaped_LO.map(lambda x: x / 255.0)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T13:02:04.107699Z","iopub.execute_input":"2022-06-06T13:02:04.108072Z","iopub.status.idle":"2022-06-06T13:02:04.641789Z","shell.execute_reply.started":"2022-06-06T13:02:04.108036Z","shell.execute_reply":"2022-06-06T13:02:04.640254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in dataset_reshaped_LO:\n    plt.axis(\"off\")\n    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2022-06-06T13:02:04.643097Z","iopub.execute_input":"2022-06-06T13:02:04.64364Z","iopub.status.idle":"2022-06-06T13:02:05.010442Z","shell.execute_reply.started":"2022-06-06T13:02:04.643595Z","shell.execute_reply":"2022-06-06T13:02:05.009584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the image quality is significantly worse.\n\nUsing a similar model set up in order to train on the lower quality image dataset.","metadata":{}},{"cell_type":"code","source":"discriminator = keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Flatten(),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ],\n    name=\"discriminator\",\n)\ndiscriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T13:02:05.013762Z","iopub.execute_input":"2022-06-06T13:02:05.017149Z","iopub.status.idle":"2022-06-06T13:02:05.106819Z","shell.execute_reply.started":"2022-06-06T13:02:05.017102Z","shell.execute_reply":"2022-06-06T13:02:05.106052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_dim = 256\n\ngenerator = keras.Sequential(\n    [\n        keras.Input(shape=(latent_dim,)),\n        layers.Dense(4 * 4 * 256),\n        layers.Reshape((4, 4, 256)),\n        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n    ],\n    name=\"generator\",\n)\ngenerator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T13:02:05.108176Z","iopub.execute_input":"2022-06-06T13:02:05.108647Z","iopub.status.idle":"2022-06-06T13:02:05.177528Z","shell.execute_reply.started":"2022-06-06T13:02:05.108611Z","shell.execute_reply":"2022-06-06T13:02:05.176714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\n\ngan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\ngan.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss_fn=keras.losses.BinaryCrossentropy(),\n)\n\ngan.fit(\n    dataset_reshaped_LO, epochs=epochs,callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T13:02:05.178608Z","iopub.execute_input":"2022-06-06T13:02:05.179043Z","iopub.status.idle":"2022-06-06T14:01:53.7684Z","shell.execute_reply.started":"2022-06-06T13:02:05.178986Z","shell.execute_reply":"2022-06-06T14:01:53.767252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, after 100 epochs the images arent particularly clear, though we are starting to see some differentiation between the images (different background colors and different shapes are beginning to form).  I will now increase the number of epochs.","metadata":{}},{"cell_type":"code","source":"epochs = 200\n\ngan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\ngan.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss_fn=keras.losses.BinaryCrossentropy(),\n)\n\ngan.fit(\n    dataset_reshaped_LO, epochs=epochs,callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T14:13:17.425046Z","iopub.execute_input":"2022-06-06T14:13:17.425408Z","iopub.status.idle":"2022-06-06T16:13:03.05543Z","shell.execute_reply.started":"2022-06-06T14:13:17.42538Z","shell.execute_reply":"2022-06-06T16:13:03.054515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are starting to see some images taking shape.  Particularly the third image above we can see a four legged animal beginning to appear.  This is interesting and indicative of likely not only needing more epochs to train but also a more sophisticated model.  I will next return to the higher quality dataset and more epochs in an attempt to see if the produced images are any more clear.  Unfortunately, after lots of trial and error, I am running out of my alloted time on the Kaggle GPUs so only running for 75 epochs (would have preferred to try to ~150-200 epochs).  I will have to revisit this next week after the weekly allowance resets.","metadata":{}},{"cell_type":"code","source":"discriminator = keras.Sequential(\n    [\n        keras.Input(shape=(64, 64, 3)),\n        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Flatten(),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ],\n    name=\"discriminator\",\n)\ndiscriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T16:14:43.423793Z","iopub.execute_input":"2022-06-06T16:14:43.424175Z","iopub.status.idle":"2022-06-06T16:14:43.476247Z","shell.execute_reply.started":"2022-06-06T16:14:43.424144Z","shell.execute_reply":"2022-06-06T16:14:43.475454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_dim = 128\n\ngenerator = keras.Sequential(\n    [\n        keras.Input(shape=(latent_dim,)),\n        layers.Dense(8 * 8 * 128),\n        layers.Reshape((8, 8, 128)),\n        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n    ],\n    name=\"generator\",\n)\ngenerator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T16:14:47.367316Z","iopub.execute_input":"2022-06-06T16:14:47.367679Z","iopub.status.idle":"2022-06-06T16:14:47.43906Z","shell.execute_reply.started":"2022-06-06T16:14:47.367648Z","shell.execute_reply":"2022-06-06T16:14:47.438255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 75\n\ngan2 = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\ngan2.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss_fn=keras.losses.BinaryCrossentropy(),\n)\n\ngan2.fit(\n    dataset_reshaped, epochs=epochs,callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T16:14:52.088721Z","iopub.execute_input":"2022-06-06T16:14:52.089085Z","iopub.status.idle":"2022-06-06T18:42:54.53064Z","shell.execute_reply.started":"2022-06-06T16:14:52.089052Z","shell.execute_reply":"2022-06-06T18:42:54.52986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we continue to see images starting to appear but they are still quite blurry.  Luckily, kaggle granted me an extension of my GPU usage of a few hours for this notebook so I am able to continue training this model for another 100 epochs.","metadata":{}},{"cell_type":"code","source":"gan2.fit(\n    dataset_reshaped, epochs=100,callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:44:33.553531Z","iopub.execute_input":"2022-06-06T18:44:33.553903Z","iopub.status.idle":"2022-06-06T21:58:24.992834Z","shell.execute_reply.started":"2022-06-06T18:44:33.553872Z","shell.execute_reply":"2022-06-06T21:58:24.991985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results and Conclusion\n\nSo, we can see we are beginning to see dogs in these images.  The model was very slow to train and its not very likely anyone would mistake these images for real dogs, but this is the beginning of a promising model nonetheless.\n\nUnfortunately, I can not submit this work to the competition since it is closed but it likely needs more work before being ready for submission anyway.\n\nI tested multiple epochs as well as different models and different initial image size/quality.  I defnitely have more work to do to understand and use GANs for image generation but this does seem like a promising start!","metadata":{}}]}